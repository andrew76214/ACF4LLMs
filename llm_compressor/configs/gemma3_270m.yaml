# Model configuration for Gemma 3 270M
  model:
    base_model: "google/gemma-3-270m"  # 或 "google/gemma-3-270m-it" 如果要用指令版本
    sequence_length: 2048              # Gemma 3 270M 支援 2K context
    max_model_len: 2048

  # Hardware configuration (270M 模型需求較低)
  hardware:
    gpu: "NVIDIA RTX 4090"            # 或你實際的 GPU
    gpu_memory_utilization: 0.5       # 270M 模型不需要太多 VRAM
    vram_limit_gb: 8                  # 8GB 就足夠了
    tensor_parallel_size: 1

  # 調整約束條件 (小模型可能準確率較低)
  constraints:
    max_accuracy_drop: 0.05           # 允許 5% 準確率下降
    p95_latency_ms: 50                # 小模型延遲較低
    max_vram_gb: 8
    baseline_latency_ms: 30

  # 只啟用需要的 agents
  agents:
    data_curation:
      enabled: true
    recipe_planner:
      enabled: false                  # 跳過複雜的 recipe planning
    quantization:
      enabled: false                  # 270M 已經很小，先不量化
    kv_longcontext:
      enabled: false
    pruning_sparsity:
      enabled: false
    distillation:
      enabled: false
    rag:
      enabled: false
    perf_carbon:
      enabled: true
    eval_safety:
      enabled: true

  # 只評測 GSM8K
  evaluation:
    mmlu:
      enabled: false
    gsm8k:
      enabled: true
      num_samples: 500                # 可以調整樣本數量
    mtbench:
      enabled: false
    safety:
      enabled: false

  # vLLM 配置
  vllm:
    api_url: "http://localhost:8000"
    port: 8000
    max_model_len: 2048
    gpu_memory_utilization: 0.5