#!/usr/bin/env python3
"""Test script for all phases (1, 2, 3)."""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

def test_phase1():
    """Test Phase 1 - MVP functionality."""
    print("\n" + "="*60)
    print("PHASE 1 - MVP TESTS")
    print("="*60)

    # Test 1: Spec Inference
    print("\n1. Testing Spec Inference...")
    from src.coordinator.spec_inference import infer_spec
    spec = infer_spec("meta-llama/Meta-Llama-3-8B-Instruct", "gsm8k")
    assert spec.model_name == "meta-llama/Meta-Llama-3-8B-Instruct"
    assert spec.model_size_gb > 0
    print(f"   ‚úì Model: {spec.model_name}")
    print(f"   ‚úì Size: {spec.model_size_gb} GB")

    # Test 2: Pareto Frontier
    print("\n2. Testing Pareto Frontier...")
    from src.coordinator.pareto import ParetoFrontier
    from src.common.schemas import EvaluationResult

    frontier = ParetoFrontier()
    result1 = EvaluationResult(
        strategy_id="strategy1",
        checkpoint_path="/test/checkpoint1",
        accuracy=0.95,
        latency_ms=100,
        memory_gb=8,
        size_gb=4,
        model_size_gb=4,
        compression_ratio=2.0,
        throughput_tokens_per_sec=100,
        evaluation_time_sec=10
    )
    result2 = EvaluationResult(
        strategy_id="strategy2",
        checkpoint_path="/test/checkpoint2",
        accuracy=0.85,
        latency_ms=50,
        memory_gb=4,
        size_gb=2,
        model_size_gb=2,
        compression_ratio=4.0,
        throughput_tokens_per_sec=200,
        evaluation_time_sec=10
    )

    frontier.add_solution("strategy1", result1)
    frontier.add_solution("strategy2", result2)

    assert len(frontier.solutions) == 2
    print(f"   ‚úì Added {len(frontier.solutions)} solutions")
    print(f"   ‚úì Both solutions are Pareto optimal")

    # Test 3: Mock Agents
    print("\n3. Testing Mock Agents...")
    from src.agents.quantization_agent import create_quantization_subagent
    from src.agents.evaluation_agent import create_evaluation_subagent

    quant_agent = create_quantization_subagent()
    eval_agent = create_evaluation_subagent()

    assert quant_agent is not None
    assert eval_agent is not None
    print(f"   ‚úì Quantization agent created")
    print(f"   ‚úì Evaluation agent created")

    print("\n‚úÖ Phase 1 tests passed!")
    return True


def test_phase2():
    """Test Phase 2 - Core features."""
    print("\n" + "="*60)
    print("PHASE 2 - CORE FEATURES TESTS")
    print("="*60)

    # Test 1: Quantization Wrappers
    print("\n1. Testing Quantization Wrappers...")
    from src.tools.quantization_wrapper import get_quantizer

    quantizer = get_quantizer("autoround")
    result = quantizer.quantize("test_model", bit_width=4)

    assert result["success"] is True
    assert "checkpoint_path" in result
    print(f"   ‚úì AutoRound quantizer: {result['compression_ratio']:.1f}x compression")

    # Test 2: Finetune Agent
    print("\n2. Testing Finetune Agent...")
    from src.agents.finetune_agent import create_finetune_subagent

    ft_agent = create_finetune_subagent()
    assert ft_agent is not None
    assert len(ft_agent.tools) > 0
    print(f"   ‚úì Finetune agent created with {len(ft_agent.tools)} tools")

    # Test 3: Pruning Agent
    print("\n3. Testing Pruning Agent...")
    from src.agents.pruning_agent import create_pruning_subagent

    prune_agent = create_pruning_subagent()
    assert prune_agent is not None
    print(f"   ‚úì Pruning agent created with {len(prune_agent.tools)} tools")

    # Test 4: Reward Function
    print("\n4. Testing Multi-Objective Reward...")
    from src.reward.reward_function import create_balanced_reward

    reward_func = create_balanced_reward()
    reward, scores = reward_func.calculate_reward(
        accuracy=0.90, latency_ms=75, memory_gb=6, size_gb=3
    )

    assert 0 <= reward <= 1
    assert "accuracy" in scores
    print(f"   ‚úì Reward calculated: {reward:.3f}")
    print(f"   ‚úì Components: {list(scores.keys())}")

    # Test 5: Strategy Generator
    print("\n5. Testing Strategy Generator...")
    from src.coordinator.strategy_generator import AdaptiveStrategyGenerator

    spec = {"model_size_gb": 8, "primary_objective": "accuracy"}
    generator = AdaptiveStrategyGenerator(spec, reward_func)

    strategy = generator.generate_next_strategy(1, [])
    assert strategy.episode_id == 1
    assert len(strategy.methods) > 0
    print(f"   ‚úì Generated strategy: {strategy.strategy_id}")
    print(f"   ‚úì Methods: {[m.value for m in strategy.methods]}")

    print("\n‚úÖ Phase 2 tests passed!")
    return True


def test_phase3():
    """Test Phase 3 - Advanced features."""
    print("\n" + "="*60)
    print("PHASE 3 - ADVANCED FEATURES TESTS")
    print("="*60)

    # Test 1: Distillation Agent
    print("\n1. Testing Distillation Agent...")
    from src.agents.distillation_agent import create_distillation_subagent

    distill_agent = create_distillation_subagent()
    assert distill_agent is not None
    print(f"   ‚úì Distillation agent created with {len(distill_agent.tools)} tools")

    # Test 2: Resource Monitor
    print("\n2. Testing Resource Monitor...")
    from src.agents.resource_monitor_agent import check_gpu_availability

    gpu_info = check_gpu_availability.invoke({})
    assert "cuda_available" in gpu_info
    assert "cpu_info" in gpu_info
    print(f"   ‚úì CUDA available: {gpu_info['cuda_available']}")
    print(f"   ‚úì CPU cores: {gpu_info['cpu_info']['cpu_count']}")

    # Test 3: Data Manager
    print("\n3. Testing Data Manager...")
    from src.agents.data_manager_agent import estimate_dataset_statistics

    stats = estimate_dataset_statistics.invoke({
        "dataset_name": "gsm8k",
        "num_samples": 10
    })
    assert "num_samples_analyzed" in stats
    print(f"   ‚úì Dataset samples: {stats['num_samples_analyzed']}")

    # Test 4: Search Algorithms
    print("\n4. Testing Search Algorithms...")
    from src.agents.search_agent import bayesian_optimization_search

    search_space = {
        "learning_rate": {"type": "log_uniform", "min": 1e-5, "max": 1e-3},
        "batch_size": {"type": "choice", "values": [8, 16, 32]}
    }

    result = bayesian_optimization_search.invoke({
        "objective_function": "accuracy",
        "search_space": search_space,
        "num_iterations": 2,
        "history": []
    })

    assert "next_parameters" in result
    print(f"   ‚úì Bayesian search suggested: {result['next_parameters']}")

    # Test 5: MLflow Tracker
    print("\n5. Testing MLflow Tracker...")
    from src.monitoring.mlflow_tracker import create_experiment_tracker

    tracker = create_experiment_tracker("test_exp", use_mlflow=False)
    run_id = tracker.start_episode(1, {"strategy_id": "test"}, {"model_name": "test"})

    tracker.log_compression_results(
        checkpoint_path="/test/path",
        compression_ratio=4.0,
        quantization_time=60.0,
        method="autoround"
    )

    tracker.end_episode("SUCCESS")
    assert run_id is not None
    print(f"   ‚úì MLflow mock tracker working")
    print(f"   ‚úì Run ID: {run_id}")

    # Test 6: Benchmark Evaluators
    print("\n6. Testing Benchmark Evaluators...")
    evaluators = []

    try:
        from src.evaluation.evaluators.gsm8k_evaluator import GSM8KEvaluator
        evaluators.append("GSM8K")
    except ImportError:
        pass

    try:
        from src.evaluation.evaluators.commonsenseqa_evaluator import CommonsenseQAEvaluator
        evaluators.append("CommonsenseQA")
    except ImportError:
        pass

    print(f"   ‚úì Available evaluators: {evaluators}")

    print("\n‚úÖ Phase 3 tests passed!")
    return True


def test_integration():
    """Test integration between components."""
    print("\n" + "="*60)
    print("INTEGRATION TESTS")
    print("="*60)

    print("\n1. Testing End-to-End Pipeline (Mock)...")
    from src.coordinator.coordinator import CompressionCoordinator

    coordinator = CompressionCoordinator(
        model_name="meta-llama/Meta-Llama-3-8B-Instruct",
        dataset="gsm8k",
        max_episodes=2,
        use_mock=True
    )

    # Run one episode
    print("   Running compression episode...")
    try:
        # Note: We can't actually run the full pipeline without DeepAgents running
        # but we can test initialization
        assert coordinator.spec is not None
        assert coordinator.pareto_frontier is not None
        print(f"   ‚úì Coordinator initialized successfully")
        print(f"   ‚úì Model: {coordinator.spec.model_name}")
        print(f"   ‚úì Target dataset: {coordinator.dataset}")
    except Exception as e:
        print(f"   ‚ö† Integration test partial (DeepAgents required for full test)")

    print("\n‚úÖ Integration tests completed!")
    return True


def main():
    """Run all tests."""
    print("="*60)
    print("AGENTIC COMPRESSION FRAMEWORK - FULL TEST SUITE")
    print("="*60)

    results = []

    # Run all test phases
    try:
        results.append(("Phase 1", test_phase1()))
    except Exception as e:
        print(f"\n‚ùå Phase 1 failed: {e}")
        results.append(("Phase 1", False))

    try:
        results.append(("Phase 2", test_phase2()))
    except Exception as e:
        print(f"\n‚ùå Phase 2 failed: {e}")
        results.append(("Phase 2", False))

    try:
        results.append(("Phase 3", test_phase3()))
    except Exception as e:
        print(f"\n‚ùå Phase 3 failed: {e}")
        results.append(("Phase 3", False))

    try:
        results.append(("Integration", test_integration()))
    except Exception as e:
        print(f"\n‚ùå Integration failed: {e}")
        results.append(("Integration", False))

    # Summary
    print("\n" + "="*60)
    print("TEST SUMMARY")
    print("="*60)

    for name, passed in results:
        status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
        print(f"{name:15} {status}")

    all_passed = all(passed for _, passed in results)

    if all_passed:
        print("\nüéâ ALL TESTS PASSED! üéâ")
    else:
        print("\n‚ö†Ô∏è  Some tests failed. Please check the output above.")

    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)