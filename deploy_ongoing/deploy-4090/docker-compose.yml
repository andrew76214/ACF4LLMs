version: "3.9"
services:
  vllm-solver-1p5b:
    image: vllm/vllm-openai:latest
    command: >
      --model ${SOLVER_MODEL}
      --quantization awq
      --dtype auto
      --max-model-len ${MAX_MODEL_LEN}
      --gpu-memory-utilization ${SOLVER_GPU_UTIL}
      --max-num-batched-tokens ${MAX_BATCHED_TOKENS}
      --max-num-seqs ${MAX_NUM_SEQS}
      --enable-chunked-prefill
      --port 8001
    environment:
      - HF_HOME=/hf-cache
    volumes:
      - /home/you/models:/models:ro
      - /home/you/.cache/huggingface:/hf-cache
    ports: ["8001:8001"]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              driver: nvidia
              count: 1
              device_ids: ["0"]  # single 4090
    restart: unless-stopped

  vllm-escalate-7b:
    image: vllm/vllm-openai:latest
    command: >
      --model ${ESCALATE_MODEL}
      --quantization awq
      --dtype auto
      --max-model-len ${MAX_MODEL_LEN}
      --gpu-memory-utilization ${ESCALATE_GPU_UTIL}
      --max-num-batched-tokens ${MAX_BATCHED_TOKENS}
      --max-num-seqs ${MAX_NUM_SEQS}
      --enable-chunked-prefill
      --port 8002
      --speculative-model ${SOLVER_MODEL}
      --speculative-model-quantization awq
      --num-speculative-tokens ${SPEC_DRAFT_TOKENS}
    environment:
      - HF_HOME=/hf-cache
    volumes:
      - /home/you/models:/models:ro
      - /home/you/.cache/huggingface:/hf-cache
    ports: ["8002:8002"]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              driver: nvidia
              count: 1
              device_ids: ["0"]
    restart: unless-stopped

  prm-server:
    build: ./prm_server
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - PRM_MODEL=/models/prm/final   # mount your trained PRM (or omit to run on CPU)
    volumes:
      - /home/you/models:/models:ro
    ports: ["8010:8010"]
    restart: unless-stopped

  pot-sandbox:
    build: ./pot_sandbox
    ports: ["8020:8020"]
    restart: unless-stopped

  router:
    build: ./router
    environment:
      - SOLVER_BASE=http://vllm-solver-1p5b:8001
      - ESCALATE_BASE=http://vllm-escalate-7b:8002
      - PRM_BASE=http://prm-server:8010
      - SANDBOX_BASE=http://pot-sandbox:8020
      - EM_TOKEN=${EM_TOKEN}
      - EM_ZONE=${EM_ZONE}
      - DEFAULT_KGCO2_PER_KWH=${DEFAULT_KGCO2_PER_KWH}
      - CRE_THRESHOLD_PCT_PER_G=${CRE_THRESHOLD_PCT_PER_G}
      - UPLIFT_7B=${UPLIFT_7B}
      - UPLIFT_GPT5=${UPLIFT_GPT5}
      - DEFAULT_7B_WH=${DEFAULT_7B_WH}
      - GPT5_G_CO2_PER_1K_TOK=${GPT5_G_CO2_PER_1K_TOK}
      - SAMPLE_INTERVAL_S=${SAMPLE_INTERVAL_S}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    ports: ["8080:8080"]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              driver: nvidia
              count: 1
              device_ids: ["0"]
    depends_on: [vllm-solver-1p5b, vllm-escalate-7b, pot-sandbox]
    restart: unless-stopped